Assignment 4
Will Potter and Parker Woodworth

1. We choose not to take into account the words that do not occur for a few reasons.  One is for the simple reason of avoiding unnecessary complexity; including these words would require the function to search the dictionary for words that were not included as well as those that were.  Another is that a long dictionary means that the probability of a given label being appropriate will be determined more heavily by what is not in it rather than what is.  For example, if a string to be classified only contains two words and is being classified agaisnt a dictionary with 10,000 words, nearly all of the determined probability will be a result of omission.

2.Examples:

"I loved that I hated it" returned positive
"this is not my favorite" returned positive
"this isn't the worst thing I've ever seen" returned negative

These three examples use some kind of false negative.  The naive algorithm is very ill equipped to deal with this type of language as it only evaluates individual words.  For examples, the word "favorite" probably has a strong correlation with positive, but the algorithm has no way to recognize its negation in this string.  In the case of "I loved that I hated it," both that string and "I hated that I loved it" return positive, so it is merely by coincidence that one of those words orderes is correct and the other is incorrect.  Again, the algorithm is not intelligent enough to understand where the negation occurs.


3. We made two major changes.  One was to add bigram analysis in addition to unigrams, the other was factoring in words entered in all caps.  Our bigram technique was a fairly naive implementation -- it does not look for specific bigrams (such as a modifier-descriptor pair) but rather just pairs bigrams as they appear.  This technique allows us to eliminate probabilities associated with colloqueil phrases and common words as the bigrams are factored in with a negative coefficient.  This makes it so our algorithm will respond more strongly to "best" than "the best."  We chose this implementation over an implementation of an algorithm to eliminate common words to avoid the necessity to compare each word against a dictionary of common words and therefore drastically increase efficiency.  The implementation of all caps is based on the assumption that a word emphasized in all caps will be particularly relevant.  For example, a reviewer would be unlikedly to use "LOVED" in a negative review.  The implemenation effectively treats a word written in all caps as having been seen one and a half times in the document.


4. 4. Our vanilla classifier had an accuracy rate of roughly 48% with movie reviews, while it had an accuracy rate of approximately 79% with our improved algorithm that accounted for bigram sequences and capitalized words. The precision for positive reviews was much better on our improved classifier, while the precision for negative reviews was much better with the vanilla classifier. Our improved classifier was much better with positive reviews, and there were more positive reviews in the dataset, thereby bringing up the overall accuracy rate to around %80.

Movie Reviews Results

ACCURACY
			| Vanilla 			| Improved
Dataset 1	| 0.475216365067	| 0.801730920535
Dataset 2	| 0.481455064194	| 0.789586305278

PRECISION

Dataset 1	| Vanilla				| Improved
positive 	| 0.38640776699			| 0.900970873786
negative 	| 0.854771784232		| 0.377593360996

Dataset 2	| Vanilla				| Improved
positive 	| 0.386809269162		| 0.894830659537
negative 	| 0.860714285714		| 0.367857142857

When we analyzed UseNet reviews, we were able to find a higher rate of accuracy and precision for all categories with our improved classifier (roughly 95%). Because we implemented bigrams, it would appear that the data is tailored to small phrases as opposed to individual words. However, our vanilla classifier performed worse, most likely indicating a higher level of complexity within the UseNet groups' data.

Auto Aviation Results
ACCURACY
			| Vanilla 			| Improved
Dataset 1	| 0.422121586387	| 0.948401262522

PRECISION
Dataset 1	| Vanilla 			| Improved
auto 		| 0.353812636166	| 0.968409586057
aviation 	| 0.538375973304	| 0.914349276974

Real Simulation Results
ACCURACY
			| Vanilla 			| Improved
Dataset 1	| 0.420915570175 | 0.952987938596

PRECISION
Dataset 1	| Vanilla 			| Improved
real 		| 0.650298301973	| 0.908673703534
simulation 	| 0.323236271253	| 0.971858510846




